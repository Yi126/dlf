{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.optim as optim\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Remove rows that contain NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Save the cleaned DataFrame back to a new CSV file\n",
    "df.to_csv('cleaned_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "def prepare_data(df):\n",
    "    # Set random seed for reproducibility\n",
    "    seed = 0\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # Separate features and labels\n",
    "    X = df.iloc[:, 1:].values  # Features (all columns except the first one)\n",
    "    y = df.iloc[:, 0].values   # Labels (the first column)\n",
    "    \n",
    "    # Convert -1 to 0 for PyTorch's BCELoss\n",
    "    y[y == -1] = 0\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    \n",
    "    # Split the dataset into training, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_tensor, y_tensor, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=43)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(X, y, method=\"none\"):\n",
    "    # Set random seed for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    if method == \"smote\":\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled_np, y_resampled_np = smote.fit_resample(X.numpy(), y.numpy().ravel())\n",
    "    elif method == \"random\":\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random_sampler = RandomOverSampler(random_state=42)\n",
    "        X_resampled_np, y_resampled_np = random_sampler.fit_resample(X.numpy(), y.numpy().ravel())\n",
    "    elif method == \"undersample\":\n",
    "        # Set random seed for reproducibility\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        np.random.seed(seed)\n",
    "        undersampler = RandomUnderSampler(random_state=42)\n",
    "        X_resampled_np, y_resampled_np = undersampler.fit_resample(X.numpy(), y.numpy().ravel())\n",
    "    elif method == \"none\":\n",
    "        X_resampled_np, y_resampled_np = X.numpy(), y.numpy().ravel()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid method: {method}\")\n",
    "    \n",
    "    return torch.tensor(X_resampled_np, dtype=torch.float32), torch.tensor(y_resampled_np, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim):\n",
    "    class MultiLayerPerceptron(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(MultiLayerPerceptron, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_dim, 128)\n",
    "            self.fc2 = nn.Linear(128, 64)\n",
    "            self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = torch.sigmoid(self.fc3(x))\n",
    "            return x\n",
    "\n",
    "    model = MultiLayerPerceptron(input_dim)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_val, y_val, epochs=1000):\n",
    "    # Set random seed for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = get_model(input_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "    best_val_f1 = 0.0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_model_weights = None\n",
    "    train_f1s_pos = []\n",
    "    train_f1s_neg = []\n",
    "    val_f1s_pos = []\n",
    "    val_f1s_neg = []\n",
    " \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_preds = (model(X_train) > 0.5).float()\n",
    "        train_f1_pos = f1_score(y_train.numpy(), train_preds.numpy(), pos_label=1)\n",
    "        train_f1_neg = f1_score(y_train.numpy(), train_preds.numpy(), pos_label=0)\n",
    "        train_f1s_pos.append(train_f1_pos)\n",
    "        train_f1s_neg.append(train_f1_neg)\n",
    "\n",
    "        train_f1_avg = (train_f1_pos + train_f1_neg) / 2.0\n",
    "        train_f1s.append(train_f1_avg)\n",
    "\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_val)\n",
    "                val_loss = criterion(val_outputs, y_val)\n",
    "                val_losses.append(val_loss.item())\n",
    "                val_preds = (val_outputs > 0.5).float()\n",
    "                val_f1_pos = f1_score(y_val.numpy(), val_preds.numpy(), pos_label=1)\n",
    "                val_f1_neg = f1_score(y_val.numpy(), val_preds.numpy(), pos_label=0)\n",
    "                val_f1s_pos.append(val_f1_pos)\n",
    "                val_f1s_neg.append(val_f1_neg)\n",
    "\n",
    "                val_f1_avg = (val_f1_pos + val_f1_neg) / 2.0\n",
    "                val_f1s.append(val_f1_avg)\n",
    "\n",
    "\n",
    "                # Model selection based on the average of positive and negative F1-scores\n",
    "                if val_f1_avg > best_val_f1:\n",
    "                    best_val_f1 = val_f1_avg\n",
    "                    best_model_weights = model.state_dict()\n",
    "\n",
    "            model.train()\n",
    "\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    metrics = {\n",
    "        \"train_f1s\": train_f1s,\n",
    "        \"val_f1s\": val_f1s,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"train_f1s_pos\": train_f1s_pos,\n",
    "        \"train_f1s_neg\": train_f1s_neg,\n",
    "        \"val_f1s_pos\": val_f1s_pos,\n",
    "        \"val_f1s_neg\": val_f1s_neg,\n",
    "    }\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test, criterion):\n",
    "    # Set random seed for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_preds = (test_outputs > 0.5).float()\n",
    "        test_f1_pos = f1_score(y_test.numpy(), test_preds.numpy(), pos_label=1)\n",
    "        test_f1_neg = f1_score(y_test.numpy(), test_preds.numpy(), pos_label=0)\n",
    "        test_recall_pos = recall_score(y_test.numpy(), test_preds.numpy(), pos_label=1)\n",
    "        test_recall_neg = recall_score(y_test.numpy(), test_preds.numpy(), pos_label=0)\n",
    "        test_precision_pos = precision_score(y_test.numpy(), test_preds.numpy(), pos_label=1)\n",
    "        test_precision_neg = precision_score(y_test.numpy(), test_preds.numpy(), pos_label=0)\n",
    "        test_accuracy = accuracy_score(y_test.numpy(), test_preds.numpy())\n",
    "        \n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "        \n",
    "    metrics = {\n",
    "        \"test_f1_pos\": test_f1_pos,\n",
    "        \"test_f1_neg\": test_f1_neg,\n",
    "        \"test_recall_pos\": test_recall_pos,\n",
    "        \"test_recall_neg\": test_recall_neg,\n",
    "        \"test_precision_pos\": test_precision_pos,\n",
    "        \"test_precision_neg\": test_precision_neg,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_loss\": test_loss.item()\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_file.csv')\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = prepare_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "# Use the desired method to process the training data\n",
    "method = \"none\" \n",
    "X_train_none, y_train_none = process_data(X_train, y_train, method=method)\n",
    "\n",
    "model_none, train_metrics_none = train(X_train_none, y_train_none, X_val, y_val)\n",
    "\n",
    "test_metrics_none = evaluate(model_none, X_test, y_test, criterion)\n",
    "\n",
    "print(\"Test F1-Score (Positive):\", test_metrics_none[\"test_f1_pos\"])\n",
    "print(\"Test F1-Score (Negative):\", test_metrics_none[\"test_f1_neg\"])\n",
    "print(\"Test Loss:\", test_metrics_none[\"test_loss\"])\n",
    "visualize_results(1000, train_metrics_none[\"train_f1s\"], train_metrics_none[\"val_f1s\"])\n",
    "plot_losses(1000, train_metrics_none[\"train_losses\"], train_metrics_none[\"val_losses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "# Using the desired method to process the training data\n",
    "method = \"undersample\" \n",
    "X_train_under, y_train_under = process_data(X_train, y_train, method=method)\n",
    "\n",
    "model_under, train_metrics_under = train(X_train_under, y_train_under, X_val, y_val)\n",
    "\n",
    "test_metrics_under = evaluate(model_under, X_test, y_test, criterion)\n",
    "\n",
    "print(\"Test F1-Score (Positive):\", test_metrics[\"test_f1_pos\"])\n",
    "print(\"Test F1-Score (Negative):\", test_metrics[\"test_f1_neg\"])\n",
    "print(\"Test Loss:\", test_metrics[\"test_loss\"])\n",
    "visualize_results(1000, train_metrics[\"train_f1s\"], train_metrics[\"val_f1s\"])\n",
    "plot_losses(1000, train_metrics[\"train_losses\"], train_metrics[\"val_losses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "# Using the desired method to process the training data\n",
    "method = \"random\" \n",
    "X_train_random, y_train_random = process_data(X_train, y_train, method=method)\n",
    "\n",
    "model_random, train_metrics_random = train(X_train_random, y_train_random, X_val, y_val)\n",
    "\n",
    "test_metrics_random = evaluate(model_random, X_test, y_test, criterion)\n",
    "\n",
    "print(\"Test F1-Score (Positive):\", test_metrics[\"test_f1_pos\"])\n",
    "print(\"Test F1-Score (Negative):\", test_metrics[\"test_f1_neg\"])\n",
    "print(\"Test Loss:\", test_metrics[\"test_loss\"])\n",
    "visualize_results(1000, train_metrics[\"train_f1s\"], train_metrics[\"val_f1s\"])\n",
    "plot_losses(1000, train_metrics[\"train_losses\"], train_metrics[\"val_losses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "# Using the desired method to process the training data\n",
    "method = \"smote\" \n",
    "X_train_smote, y_train_smote = process_data(X_train, y_train, method=method)\n",
    "\n",
    "model_smote, train_metrics_smote = train(X_train_smote, y_train_smote, X_val, y_val)\n",
    "\n",
    "test_metrics_smote = evaluate(model_smote, X_test, y_test, criterion)\n",
    "\n",
    "print(\"Test F1-Score (Positive):\", test_metrics[\"test_f1_pos\"])\n",
    "print(\"Test F1-Score (Negative):\", test_metrics[\"test_f1_neg\"])\n",
    "print(\"Test Loss:\", test_metrics[\"test_loss\"])\n",
    "visualize_results(1000, train_metrics[\"train_f1s\"], train_metrics[\"val_f1s\"])\n",
    "plot_losses(1000, train_metrics[\"train_losses\"], train_metrics[\"val_losses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_comparison_table_with_avg(*evaluation_results):\n",
    "    methods = [\"none\", \"smote\", \"random\", \"undersample\"]\n",
    "    \n",
    "    # Ensure we have results for each method\n",
    "    assert len(evaluation_results) == len(methods), \"Provide evaluation results for each method.\"\n",
    "    \n",
    "    # Extracting the metrics we need\n",
    "    rows = []\n",
    "    for method, result in zip(methods, evaluation_results):\n",
    "        avg_recall = (result[\"test_recall_pos\"] + result[\"test_recall_neg\"]) / 2\n",
    "        avg_precision = (result[\"test_precision_pos\"] + result[\"test_precision_neg\"]) / 2\n",
    "        avg_f1 = (result[\"test_f1_pos\"] + result[\"test_f1_neg\"]) / 2\n",
    "        \n",
    "        row = {\n",
    "            \"Method\": method,\n",
    "            \"F1 (Positive)\": result[\"test_f1_pos\"],\n",
    "            \"F1 (Negative)\": result[\"test_f1_neg\"],\n",
    "            \"Avg F1\": avg_f1,\n",
    "            \"Recall (Positive)\": result[\"test_recall_pos\"],\n",
    "            \"Recall (Negative)\": result[\"test_recall_neg\"],\n",
    "            \"Avg Recall\": avg_recall,\n",
    "            \"Precision (Positive)\": result[\"test_precision_pos\"],\n",
    "            \"Precision (Negative)\": result[\"test_precision_neg\"],\n",
    "            \"Avg Precision\": avg_precision,\n",
    "            \"Accuracy\": result[\"test_accuracy\"],\n",
    "        }\n",
    "        rows.append(row)\n",
    "        \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def save_comparison_table_to_csv(filename, *evaluation_results):\n",
    "    # Generate the comparison table\n",
    "    df = generate_comparison_table_with_avg(*evaluation_results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    return f\"Comparison table saved to {filename}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('comparison_table.csv')\n",
    "\n",
    "# Set the style of the visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Draw a bar plot of 'Avg F1' score for each method\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=df['Method'], y=df['Avg F1'], palette=\"Blues_d\")\n",
    "plt.title('Average F1 Score by Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(epochs, train_losses, val_losses):\n",
    "    # Set random seed for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(epochs), train_losses, label='Training Loss')\n",
    "    plt.plot(range(0, epochs, 100), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.yscale('log')  # Set y-axis scale to logarithmic\n",
    "    plt.title('Log Loss vs. Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
