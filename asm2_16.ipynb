{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imbalanced subset of the dataset\n",
    "def create_imbalanced_subset(dataset, imbalance_ratio=0.01, seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # Each class in CIFAR-100 has 500 samples\n",
    "    samples_per_class = 500\n",
    "    \n",
    "    # Calculate the start and end index for class 0\n",
    "    end_idx = int(samples_per_class * imbalance_ratio)\n",
    "    \n",
    "    # Use slicing to directly obtain the required number of samples for class 0\n",
    "    main_class_subset_data = dataset.data[:end_idx]\n",
    "    main_class_subset_labels = [0] * end_idx\n",
    "    \n",
    "    # Get samples for other classes\n",
    "    other_data = dataset.data[samples_per_class:]\n",
    "    other_labels = dataset.targets[samples_per_class:]\n",
    "    \n",
    "    # Merge the data\n",
    "    imbalanced_data = np.concatenate([main_class_subset_data, other_data])\n",
    "    imbalanced_labels = main_class_subset_labels + other_labels\n",
    "    \n",
    "    return imbalanced_data, imbalanced_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VGG block\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VGG network\n",
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 3\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "    *conv_blks, nn.Flatten(),\n",
    "    nn.Linear(512, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg11():\n",
    "    conv_arch11 = [(1, 64), (1, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    model = vgg(conv_arch11)\n",
    "    model.name = \"VGG11\"\n",
    "    return model\n",
    "\n",
    "def vgg13():\n",
    "    conv_arch13 = [(2, 64), (2, 128), (2, 256), (2, 512), (2, 512)]\n",
    "    model = vgg(conv_arch13)\n",
    "    model.name = \"VGG13\"\n",
    "    return model\n",
    "\n",
    "def vgg16():\n",
    "    conv_arch16 = [(2, 64), (2, 128), (3, 256), (3, 512), (3, 512)]\n",
    "    model = vgg(conv_arch16)\n",
    "    model.name = \"VGG16\"\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, n_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.view(inputs.size(0), -1)  # Flatten the input data\n",
    "            inputs = inputs.float()  # Ensure the data is float type\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_specific_model(processed_trainset, model, imbalance_ratios, batch_size, n_epochs, lr, device):\n",
    "    avg_losses = []  # List to store average loss for each imbalance ratio\n",
    "    \n",
    "    for ratio in imbalance_ratios:\n",
    "        X, y = zip(*processed_trainset)\n",
    "        fold_losses = []  # List to store loss for each fold\n",
    "        \n",
    "        # Split the data into training and validation sets for each fold\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            train_fold = [(X[i], y[i]) for i in train_idx]\n",
    "            \n",
    "            train_fold_loader = DataLoader(train_fold, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            # Clone the model to ensure fresh training for each fold\n",
    "            net = copy.deepcopy(model)\n",
    "            net.to(device)\n",
    "            \n",
    "            train_loss = train_model(net, train_fold_loader, n_epochs, lr, device)\n",
    "            \n",
    "            fold_losses.append(train_loss)  # Store the last loss value for this fold\n",
    "        \n",
    "        # Calculate the average loss of cross-validation\n",
    "        avg_train_loss = sum(fold_losses) / len(fold_losses)\n",
    "        avg_losses.append(avg_train_loss)  # Store the average loss for this imbalance ratio   \n",
    "    \n",
    "    return avg_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(36, padding=4),\n",
    "        transforms.CenterCrop(32),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_same_shape_and_range(data, original_shape):\n",
    "    # Ensure the shape is the same as the original shape\n",
    "    data = data.reshape(original_shape)\n",
    "    \n",
    "    # Find the minimum and maximum values of the data\n",
    "    min_value = np.amin(data)\n",
    "    max_value = np.amax(data)\n",
    "    \n",
    "    # Scale the data to the [0, 255] range\n",
    "    scaled_data = ((data - min_value) / (max_value - min_value) * 255).astype(np.uint8)\n",
    "    \n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(conv_arch):\n",
    "    return vgg(conv_arch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(method, trainset, main_class=0, imbalance_ratio=0.01, seed=42):\n",
    "    \"\"\"\n",
    "    Preprocess the data using the specified method.\n",
    "    \n",
    "    Parameters:\n",
    "    - method: The data augmentation method to use. One of ['smote', 'random_over', 'random_under'].\n",
    "    - trainset: The training dataset.\n",
    "    - main_class: The class to be imbalanced.\n",
    "    - imbalance_ratio: The ratio of the main class in the imbalanced dataset.\n",
    "    - seed: Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - Processed dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the imbalanced data and labels\n",
    "    imbalanced_data, imbalanced_labels = create_imbalanced_subset(trainset, imbalance_ratio)\n",
    "\n",
    "    # Convert the data to numpy arrays\n",
    "    X = np.array(imbalanced_data)\n",
    "    y = np.array(imbalanced_labels)\n",
    "\n",
    "    # Apply the specified method\n",
    "    if method == 'smote':\n",
    "        smote = SMOTE(random_state=seed)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X.reshape(X.shape[0], -1), y)\n",
    "    elif method == 'random_over':\n",
    "        ros = RandomOverSampler(random_state=seed)\n",
    "        X_resampled, y_resampled = ros.fit_resample(X.reshape(X.shape[0], -1), y)\n",
    "    elif method == 'random_under':\n",
    "        rus = RandomUnderSampler(random_state=seed)\n",
    "        X_resampled, y_resampled = rus.fit_resample(X.reshape(X.shape[0], -1), y)\n",
    "    elif method == 'none':\n",
    "        X_resampled, y_resampled = X.reshape(X.shape[0], -1), y\n",
    "        \n",
    "    # Convert the resampled data back to the original shape\n",
    "    X_resampled = X_resampled.reshape(X_resampled.shape[0], 3, 32, 32)\n",
    "    X_resampled = [torch.tensor(x, dtype=torch.float32) for x in X_resampled]\n",
    "    \n",
    "    # Combine the resampled features and labels into a dataset\n",
    "    processed_data = list(zip(X_resampled, y_resampled))\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(testset, models, methods, imbalance_ratios, batch_size, device):\n",
    "    \"\"\"\n",
    "    Test the models on imbalanced datasets.\n",
    "    \n",
    "    Args:\n",
    "    - testset: The original test dataset.\n",
    "    - models: A list of models to test.\n",
    "    - methods: A list of methods to apply (e.g., ['smote', 'random_over', 'random_under']).\n",
    "    - imbalance_ratios: A list of imbalance ratios to create subsets.\n",
    "    - batch_size: Batch size for DataLoader.\n",
    "    - device: Device to run the model on ('cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary containing average accuracy and F1 score for each model-method combination.\n",
    "    - A dictionary containing accuracy and F1 score for each imbalance ratio for each model-method combination.\n",
    "    \"\"\"\n",
    "    \n",
    "    avg_results = {}\n",
    "    detailed_results = {}\n",
    "    \n",
    "    for model in models:\n",
    "        for method in methods:\n",
    "            all_accuracies = []\n",
    "            all_f1_scores = []\n",
    "            \n",
    "            for ratio in imbalance_ratios:\n",
    "                # Create imbalanced subset\n",
    "                subset_data, subset_labels = create_imbalanced_subset(testset, imbalance_ratio=ratio)\n",
    "                test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "                \n",
    "                model.to(device)\n",
    "                model.eval()  # Set the model to evaluation mode\n",
    "                \n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for data, labels in test_loader:\n",
    "                        data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
    "                        data, labels = data.to(device), labels.to(device)\n",
    "                        outputs = model(data)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        \n",
    "                        all_preds.extend(predicted.cpu().numpy())\n",
    "                        all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # Calculate accuracy and F1 score\n",
    "                accuracy = accuracy_score(all_labels, all_preds)\n",
    "                f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "                \n",
    "                all_accuracies.append(accuracy)\n",
    "                all_f1_scores.append(f1)\n",
    "                \n",
    "                # Store detailed results\n",
    "                key = f\"{model.name}_{method}_{ratio}\"\n",
    "                detailed_results[key] = {'accuracy': accuracy, 'f1_score': f1}\n",
    "            \n",
    "            # Calculate average accuracy and F1 score\n",
    "            avg_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
    "            avg_f1 = sum(all_f1_scores) / len(all_f1_scores)\n",
    "            \n",
    "            avg_key = f\"{model.name}_{method}\"\n",
    "            avg_results[avg_key] = {'avg_accuracy': avg_accuracy, 'avg_f1_score': avg_f1}\n",
    "    \n",
    "    return avg_results, detailed_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "IMBALANCE_RATIOS = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "BATCH_SIZE = 1024\n",
    "N_EPOCHS = 50\n",
    "LR = 0.001\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = vgg16()\n",
    "method = 'smote'\n",
    "\n",
    "# Load datasets\n",
    "trainset, testset = load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "processed_trainset = preprocess_data(method, trainset)\n",
    "\n",
    "# Train the model and get average losses\n",
    "avg_losses = train_specific_model(processed_trainset, model, IMBALANCE_RATIOS, BATCH_SIZE, N_EPOCHS, LR, DEVICE)\n",
    "\n",
    "# Save the model state\n",
    "torch.save(model.state_dict(), f'/Project/Yi/VGG16_CIFAR100_{method}.pth')\n",
    "\n",
    "# Save the average losses to CSV\n",
    "df_avg_losses = pd.DataFrame(avg_losses, index=IMBALANCE_RATIOS, columns=[f\"{model.name}_{method}\"])\n",
    "df_avg_losses.to_csv('average_losses16.csv', mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "IMBALANCE_RATIOS = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "BATCH_SIZE = 1024\n",
    "N_EPOCHS = 150\n",
    "LR = 0.01\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = vgg16()\n",
    "method = 'random_over'\n",
    "\n",
    "# Load datasets\n",
    "trainset, testset = load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "processed_trainset = preprocess_data(method, trainset)\n",
    "\n",
    "# Train the model and get average losses\n",
    "avg_losses = train_specific_model(processed_trainset, model, IMBALANCE_RATIOS, BATCH_SIZE, N_EPOCHS, LR, DEVICE)\n",
    "\n",
    "# Save the model state\n",
    "torch.save(model.state_dict(), f'/Project/Yi/VGG16_CIFAR100_{method}.pth')\n",
    "\n",
    "# Save the average losses to CSV\n",
    "df_avg_losses = pd.DataFrame(avg_losses, index=IMBALANCE_RATIOS, columns=[f\"{model.name}_{method}\"])\n",
    "df_avg_losses.to_csv('average_losses16.csv', mode='a')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "IMBALANCE_RATIOS = [ 0.01, 0.05, 0.1, 0.5, 1]\n",
    "BATCH_SIZE = 1024\n",
    "N_EPOCHS = 150\n",
    "LR = 0.01\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = vgg16()\n",
    "method = 'random_under'\n",
    "\n",
    "# Load datasets\n",
    "trainset, testset = load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "processed_trainset = preprocess_data(method, trainset)\n",
    "\n",
    "# Train the model and get average losses\n",
    "avg_losses = train_specific_model(processed_trainset, model, IMBALANCE_RATIOS, BATCH_SIZE, N_EPOCHS, LR, DEVICE)\n",
    "\n",
    "# Save the model state\n",
    "torch.save(model.state_dict(), f'/Project/Yi/VGG16_CIFAR100_{method}.pth')\n",
    "\n",
    "# Save the average losses to CSV\n",
    "df_avg_losses = pd.DataFrame(avg_losses, index=IMBALANCE_RATIOS, columns=[f\"{model.name}_{method}\"])\n",
    "df_avg_losses.to_csv('average_losses16.csv', mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "IMBALANCE_RATIOS = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "BATCH_SIZE = 1024\n",
    "N_EPOCHS = 150\n",
    "LR = 0.01\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = vgg16()\n",
    "method = 'none'\n",
    "\n",
    "# Load datasets\n",
    "trainset, testset = load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "processed_trainset = preprocess_data(method, trainset)\n",
    "\n",
    "# Train the model and get average losses\n",
    "avg_losses = train_specific_model(processed_trainset, model, IMBALANCE_RATIOS, BATCH_SIZE, N_EPOCHS, LR, DEVICE)\n",
    "\n",
    "# Save the model state\n",
    "torch.save(model.state_dict(), f'/Project/Yi/VGG16_CIFAR100_{method}.pth')\n",
    "\n",
    "# Save the average losses to CSV\n",
    "df_avg_losses = pd.DataFrame(avg_losses, index=IMBALANCE_RATIOS, columns=[f\"{model.name}_{method}\"])\n",
    "df_avg_losses.to_csv('average_losses16.csv', mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n",
      "/tmp/ipykernel_47962/1157159054.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data).permute(0, 3, 1, 2).float() / 255.0  # Convert and normalize\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries to store results\n",
    "all_avg_results = {}\n",
    "all_detailed_results = {}\n",
    "IMBALANCE_RATIOS = [ 0.01, 0.05, 0.1, 0.5, 1]\n",
    "trainset, testset = load_data()\n",
    "BATCH_SIZE = 1024\n",
    "N_EPOCHS = 150\n",
    "LR = 0.01\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# DataFrames to store results\n",
    "df_avg_results = pd.DataFrame(columns=['avg_accuracy', 'avg_f1_score'])\n",
    "df_detailed_results = pd.DataFrame(columns=IMBALANCE_RATIOS)\n",
    "\n",
    "# Load the model\n",
    "model = vgg16()\n",
    "\n",
    "# Iterate over each method for testing\n",
    "METHODS = ['smote', 'random_over', 'random_under', 'none']\n",
    "for method in METHODS:\n",
    "    # Load the model state\n",
    "    key = f\"{model.name}_{method}\"\n",
    "    model.load_state_dict(torch.load(f'/Project/Yi/VGG16_CIFAR100_{method}.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    # Test the current model-method combination\n",
    "    avg_results, detailed_results = test_model(testset, [model], [method], IMBALANCE_RATIOS, BATCH_SIZE, DEVICE)\n",
    "    \n",
    "    # Store results\n",
    "    all_avg_results[key] = avg_results[key]\n",
    "    all_detailed_results[key] = detailed_results\n",
    "\n",
    "    # Save results to the DataFrames and then to CSV\n",
    "    df_avg_results.loc[key] = [avg_results[key]['avg_accuracy'], avg_results[key]['avg_f1_score']]\n",
    "    for ratio in IMBALANCE_RATIOS:\n",
    "        detailed_key = f\"{key}_{ratio}\"\n",
    "        df_detailed_results.at[key, ratio] = detailed_results[detailed_key]['accuracy']\n",
    "    \n",
    "    df_avg_results.to_csv('average_results16.csv', mode='a')\n",
    "    df_detailed_results.to_csv('detailed_results16.csv', mode='a')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
